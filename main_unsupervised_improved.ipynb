{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tkinter import font\n",
    "import tkinter as tk\n",
    "import pandas as pd\n",
    "from errno import EEXIST\n",
    "from os import makedirs, path\n",
    "import numpy as np\n",
    "import os\n",
    "from stat import S_IREAD, S_IRGRP, S_IROTH\n",
    "# from PIL import Image\n",
    "import time\n",
    "from statistics import mean\n",
    "from abstraction import AMDP\n",
    "from maze_env_general import Maze\n",
    "from RL_brain_fast import WatkinsQLambda\n",
    "from gensim_operation_online import GensimOperator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# abstraction_mode = [None, (3, 3), (4, 4), (5, 5), (7, 7), (9, 9), None]   # 可修改\n",
    "abstraction_mode = [None]  # 可修改\n",
    "env = Maze(maze='low_connectivity')  # initialize env 可修改\n",
    "\n",
    "\n",
    "num_of_actions = 4\n",
    "num_of_experiments = len(abstraction_mode)\n",
    "lr = 0.1\n",
    "lam = 0.9\n",
    "gamma = 0.99\n",
    "omega = 20\n",
    "epsilon = 1   # probability for choosing random action  #可修改\n",
    "num_randomwalk_episodes = 400\n",
    "num_of_episodes = num_randomwalk_episodes + 4005     # 可修改\n",
    "num_of_repetitions = 2      # 可修改\n",
    "max_move_count = 10000\n",
    "min_length_to_save_as_path = 400\n",
    "\n",
    "config = {\n",
    "    'maze': env.maze_name,\n",
    "    'mode': 'random+biased_paths',\n",
    "    'ep': num_of_episodes,\n",
    "    'max_move_count': max_move_count,\n",
    "    'min_length_to_save': min_length_to_save_as_path,\n",
    "    'representation_size': 64,\n",
    "    'window': 20,\n",
    "    'kmeans_clusters': [8,12,20],\n",
    "    'package': 'sklearn'\n",
    "}\n",
    "# fpath_paths = f\"paths/{config['maze']}/{config['mode']}/ep_{config['ep']}_maxmove_{config['max_move_count']}_length_\" \\\n",
    "#               f\"{config['min_length_to_save']}+.txt\"\n",
    "# folder_paths = f\"paths/{config['maze']}/{config['mode']}\"\n",
    "#\n",
    "# # fpath_paths2 = f\"paths/{config['maze']}/{config['mode']}/ep_{config['ep']}_maxmove_{config['max_move_count']}_length_\" \\\n",
    "# #               f\"{config['min_length_to_save']}+.txt\"\n",
    "# # folder_paths2 = f\"paths/{config['maze']}/{config['mode']}\"\n",
    "# #\n",
    "# # fpath_paths3 = f\"paths/{config['maze']}/{config['mode']}/ep_{config['ep']}_maxmove_{config['max_move_count']}_length_\" \\\n",
    "# #               f\"{config['min_length_to_save']}+.txt\"\n",
    "# # folder_paths3 = f\"paths/{config['maze']}/{config['mode']}\"\n",
    "#\n",
    "# fpath_embedding = f\"embeddings/{config['maze']}/{config['mode']}/ep_{config['ep']}_maxmove_{config['max_move_count']}_length_\" \\\n",
    "#                   f\"{config['min_length_to_save']}+/s{config['representation_size']}_w{config['window']}.embedding\"\n",
    "# folder_embedding = f\"embeddings/{config['maze']}/{config['mode']}/ep_{config['ep']}_maxmove_{config['max_move_count']}_length_\" \\\n",
    "#                    f\"{config['min_length_to_save']}+\"\n",
    "#\n",
    "# fpath_cluster_layout = f\"cluster_layout/{config['maze']}/{config['mode']}/ep{config['ep']}_maxmove{config['max_move_count']}_length\" \\\n",
    "#                        f\"{config['min_length_to_save']}+/s{config['representation_size']}_w{config['window']}\" \\\n",
    "#                        f\"_kmeans{config['kmeans_clusters']}_package_{config['package']}.cluster\"\n",
    "folder_cluster_layout = f\"cluster_layout/{config['maze']}/{config['mode']}/ep{config['ep']}_maxmove{config['max_move_count']}_length\" \\\n",
    "                        f\"{config['min_length_to_save']}+\"\n",
    "# if not os.path.isdir(folder_paths):\n",
    "#     makedirs(folder_paths)\n",
    "# if not os.path.isdir(folder_embedding):\n",
    "#     makedirs(folder_embedding)\n",
    "if not os.path.isdir(folder_cluster_layout):\n",
    "    makedirs(folder_cluster_layout)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for ploting\n",
    "solve_amdp_time_experiments_repetitions = []\n",
    "simulation_time_experiments_repetitions = []\n",
    "reward_list_episodes_experiments_repetitions = []\n",
    "flags_list_episodes_experiments_repetitions = []\n",
    "move_count_episodes_experiments_repetitions = []\n",
    "path_episodes_experiments_repetitions = []\n",
    "\n",
    "\n",
    "for rep in range(0, num_of_repetitions):\n",
    "\n",
    "    solve_amdp_time_experiments = []\n",
    "    simulation_time_experiments = []\n",
    "    reward_list_episodes_experiments = []\n",
    "    flags_list_episodes_experiments = []\n",
    "    move_count_episodes_experiments = []\n",
    "    path_episodes_experiments = []\n",
    "\n",
    "    # move_count = 0\n",
    "    # totalMoveCount = 0\n",
    "    # maxflag = 0\n",
    "    # maxIndex = 0\n",
    "    # flagCount = 0\n",
    "    # reward_list_episodes = []\n",
    "    # flags_list_episodes = []\n",
    "\n",
    "    for e in range(0, num_of_experiments):\n",
    "        print('===================num_of_experiments:', e, \"Repetition:\", rep)\n",
    "        start2 = time.time()\n",
    "        move_count = 0\n",
    "        totalMoveCount = 0\n",
    "        maxflag = 0\n",
    "        maxIndex = 0\n",
    "        flagCount = 0\n",
    "        reward_list_episodes = []\n",
    "        flags_list_episodes = []\n",
    "        move_count_episodes = []\n",
    "        path_episodes = []\n",
    "        solve_amdp_time_phases = []\n",
    "        all_path_lengths = []\n",
    "\n",
    "        agent = WatkinsQLambda(env.size, num_of_actions, env, epsilon, lr, gamma, lam)  ## resets the agent\n",
    "        gensim_opt = GensimOperator(path_episodes, env)\n",
    "\n",
    "        print(\"Begin Training:\")\n",
    "        for ep in range(0, num_of_episodes):\n",
    "            if ep == num_randomwalk_episodes:\n",
    "                # print(\"path_episodes:\",path_episodes)\n",
    "                min_length_to_save_as_path -= 100\n",
    "                agent.epsilon = 0.5\n",
    "                gensim_opt.sentences = path_episodes\n",
    "                gensim_opt.get_clusterlayout_from_paths(size=64, window=20, clusters=config['kmeans_clusters'][0], package=config['package'])\n",
    "                fpath_cluster_layout = folder_cluster_layout + f\"/s{config['representation_size']}_w{config['window']}\" \\\n",
    "                        f\"_kmeans{config['kmeans_clusters'][0]}_{config['package']}.cluster\"\n",
    "                gensim_opt.write_cluster_layout(fpath_cluster_layout)\n",
    "\n",
    "                amdp = AMDP(env=env, tiling_mode=None, dw_clt_layout=gensim_opt.cluster_layout)\n",
    "                start1 = time.time()\n",
    "                amdp.solveAbstraction()\n",
    "                end1 = time.time()\n",
    "                solve_amdp_time_phases.append(end1 - start1)\n",
    "            elif ep == num_randomwalk_episodes + 999:\n",
    "                min_length_to_save_as_path -= 100\n",
    "                # print(\"path_episodes:\",path_episodes)\n",
    "                gensim_opt.sentences = path_episodes\n",
    "                gensim_opt.get_clusterlayout_from_paths(size=64, window=20, clusters=config['kmeans_clusters'][1], package=config['package'])\n",
    "                fpath_cluster_layout = folder_cluster_layout + f\"/s{config['representation_size']}_w{config['window']}\" \\\n",
    "                        f\"_kmeans{config['kmeans_clusters'][1]}_{config['package']}.cluster\"\n",
    "                gensim_opt.write_cluster_layout(fpath_cluster_layout)\n",
    "\n",
    "                amdp = AMDP(env=env, tiling_mode=None, dw_clt_layout=np.array(gensim_opt.cluster_layout))\n",
    "                start1 = time.time()\n",
    "                amdp.solveAbstraction()\n",
    "                end1 = time.time()\n",
    "                solve_amdp_time_phases.append(end1 - start1)\n",
    "            elif ep == num_randomwalk_episodes + 1999:\n",
    "                min_length_to_save_as_path -= 100\n",
    "                # print(\"path_episodes:\",path_episodes)\n",
    "                gensim_opt.sentences = path_episodes\n",
    "                gensim_opt.get_clusterlayout_from_paths(size=64, window=20, clusters=config['kmeans_clusters'][2], package=config['package'])\n",
    "                fpath_cluster_layout = folder_cluster_layout + f\"/s{config['representation_size']}_w{config['window']}\" \\\n",
    "                        f\"_kmeans{config['kmeans_clusters'][2]}_{config['package']}.cluster\"\n",
    "                gensim_opt.write_cluster_layout(fpath_cluster_layout)\n",
    "\n",
    "                amdp = AMDP(env=env, tiling_mode=None, dw_clt_layout=np.array(gensim_opt.cluster_layout))\n",
    "                start1 = time.time()\n",
    "                amdp.solveAbstraction()\n",
    "                end1 = time.time()\n",
    "                solve_amdp_time_phases.append(end1 - start1)\n",
    "\n",
    "            if (ep + 1) % 100 == 0:\n",
    "                print(\"episode_100:\", ep)\n",
    "\n",
    "            env.reset()\n",
    "            agent.resetEligibility()  # 可以修改\n",
    "\n",
    "            if ep >= num_randomwalk_episodes and ep-num_randomwalk_episodes > (num_of_episodes-num_randomwalk_episodes)/10: # 可修改\n",
    "                agent.epsilon -= (0.5) / (num_of_episodes-num_randomwalk_episodes)   ##reduce exploration over time.   # 可修改\n",
    "\n",
    "            episode_reward = 0\n",
    "            move_count = 0\n",
    "            a = agent.policy(env.state, env.actions(env.state))\n",
    "            path = [str((env.state[0], env.state[1]))]\n",
    "\n",
    "            while not env.isTerminal(env.state):\n",
    "                # print(\"env.isTerminal(env.state):\",env.isTerminal(env.state))\n",
    "                move_count += 1\n",
    "\n",
    "                if ep < num_randomwalk_episodes:\n",
    "                    if move_count > max_move_count:\n",
    "                        break\n",
    "                    else:\n",
    "                        new_state = env.step(env.state, a)\n",
    "                        a_prime = agent.policy(new_state, env.actions(new_state))\n",
    "                        path.append(str((new_state[0], new_state[1])))\n",
    "                else:\n",
    "                    ##Select action using policy\n",
    "                    abstract_state = amdp.getAbstractState(env.state)\n",
    "                    new_state = env.step(env.state, a)\n",
    "                    new_abstract_state = amdp.getAbstractState(new_state)\n",
    "                    # print(new_state, new_abstract_state)\n",
    "\n",
    "                    a_prime = agent.policy(new_state, env.actions(new_state))  ##Greedy next-action selected\n",
    "                    a_star = agent.policyNoRand(new_state, env.actions(new_state))\n",
    "                    ## Optimal next action ---- comparison of the two required for Watkins Q-lambda\n",
    "\n",
    "                    r = env.reward(env.state, a, new_state)  ## ground level reward\n",
    "                    episode_reward += r\n",
    "\n",
    "                    # if abstract_state[0] not in gensim_opt.cluster_labels or new_abstract_state[0] not in gensim_opt.cluster_labels:\n",
    "                    if False:\n",
    "                        shaping = 0\n",
    "                        agent.learn(env.state, a, new_state, a_prime, a_star, r + shaping)  # 可以修改\n",
    "                        path.append(str((new_state[0], new_state[1])))\n",
    "                    else:\n",
    "                        value_new_abstract_state = amdp.getValue(new_abstract_state)\n",
    "                        value_abstract_state = amdp.getValue(abstract_state)\n",
    "                        # print(\"type(value_abstract_state):\",type(value_abstract_state))\n",
    "                        shaping = gamma * value_new_abstract_state * omega - value_abstract_state * omega\n",
    "                        agent.learn(env.state, a, new_state, a_prime, a_star, r + shaping)  # 可以修改\n",
    "                        path.append(str((new_state[0], new_state[1])))\n",
    "\n",
    "                env.state = new_state\n",
    "                a = a_prime\n",
    "            # next steps actions and states set.\n",
    "\n",
    "            ############# Keep Track of Stuff for each ep ################\n",
    "            # flagCount += env.flags_collected\n",
    "            # totalMoveCount += move_count\n",
    "            reward_list_episodes.append(episode_reward)\n",
    "            flags_list_episodes.append(env.flags_collected)\n",
    "            move_count_episodes.append(move_count)\n",
    "            all_path_lengths.append(len(path))\n",
    "            if len(path) > min_length_to_save_as_path:\n",
    "                path_episodes.append(path)\n",
    "        # =====================\n",
    "        print(\"last state:\",env.state)\n",
    "        print(\"all_path_lengths:\", all_path_lengths)\n",
    "        print(\"len of all_path_lengths:\", len(all_path_lengths))\n",
    "\n",
    "        mean_unreduced = mean(all_path_lengths[:num_randomwalk_episodes])\n",
    "        saved_path_lengths = [x for x in all_path_lengths[:num_randomwalk_episodes] if x > min_length_to_save_as_path]\n",
    "        if len(saved_path_lengths)>0:\n",
    "            mean_reduced = mean(saved_path_lengths)\n",
    "            print(\"avg saved_path_lengths of random walk period:\", mean_reduced, len(saved_path_lengths))\n",
    "        print(\"avg all_path_lengths of random walk period:\",mean_unreduced)\n",
    "\n",
    "        mean_unreduced = mean(all_path_lengths[num_randomwalk_episodes:num_randomwalk_episodes+999])\n",
    "        saved_path_lengths = [x for x in all_path_lengths[num_randomwalk_episodes:num_randomwalk_episodes+999] if x > min_length_to_save_as_path]\n",
    "        if len(saved_path_lengths)>0:\n",
    "            mean_reduced = mean(saved_path_lengths)\n",
    "            print(\"avg saved_path_lengths of biased walk period1:\", mean_reduced, len(saved_path_lengths))\n",
    "        print(\"avg all_path_lengths of biased walk period1:\",mean_unreduced)\n",
    "\n",
    "        mean_unreduced = mean(all_path_lengths[num_randomwalk_episodes+999:num_randomwalk_episodes+1999])\n",
    "        saved_path_lengths = [x for x in all_path_lengths[num_randomwalk_episodes+999:num_randomwalk_episodes+1999] if x > min_length_to_save_as_path]\n",
    "        if len(saved_path_lengths)>0:\n",
    "            mean_reduced = mean(saved_path_lengths)\n",
    "            print(\"avg saved_path_lengths of biased walk period2:\", mean_reduced, len(saved_path_lengths))\n",
    "        print(\"avg all_path_lengths of biased walk period2:\",mean_unreduced)\n",
    "\n",
    "        mean_unreduced = mean(all_path_lengths[num_randomwalk_episodes+1999:])\n",
    "        saved_path_lengths=[x for x in all_path_lengths[num_randomwalk_episodes+1999:] if x > min_length_to_save_as_path]\n",
    "        if len(saved_path_lengths)>0:\n",
    "            mean_reduced = mean(saved_path_lengths)\n",
    "            print(\"avg saved_path_lengths of biased walk period3:\", mean_reduced, len(saved_path_lengths))\n",
    "        print(\"avg all_path_lengths of biased walk period3:\",mean_unreduced)\n",
    "\n",
    "        #=====================\n",
    "        solve_amdp_time_experiments.append(solve_amdp_time_phases)\n",
    "        reward_list_episodes_experiments.append(reward_list_episodes)\n",
    "        flags_list_episodes_experiments.append(flags_list_episodes)\n",
    "        move_count_episodes_experiments.append(move_count_episodes)\n",
    "        path_episodes_experiments.append(path_episodes)\n",
    "\n",
    "        end2 = time.time()\n",
    "        simulation_time_experiments.append(end2 - start2)\n",
    "        print(\"agent.epsilon:\",agent.epsilon)\n",
    "\n",
    "\n",
    "    solve_amdp_time_experiments_repetitions.append(solve_amdp_time_experiments)\n",
    "    simulation_time_experiments_repetitions.append(simulation_time_experiments)\n",
    "    flags_list_episodes_experiments_repetitions.append(flags_list_episodes_experiments)\n",
    "    reward_list_episodes_experiments_repetitions.append(reward_list_episodes_experiments)\n",
    "    move_count_episodes_experiments_repetitions.append(move_count_episodes_experiments)\n",
    "    path_episodes_experiments_repetitions.append(path_episodes_experiments)\n",
    "\n",
    "print(flags_list_episodes_experiments_repetitions)\n",
    "print(move_count_episodes_experiments_repetitions)\n",
    "print(np.sum(np.array(move_count_episodes_experiments_repetitions)))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# labs = [\"True\", \"3x3\", \"4x4\", \"5x5\", \"7x7\", \"9x9\", \"10x10\", \"None\"]     # 可修改\n",
    "labs = [\"biased\"]  # 可修改\n",
    "\n",
    "# output_dir = \"FExperiments/\" + env.maze_name + \"qLambdaAlpha\" + str(lr) + \"Gamma\" + str(gamma) + \"Lambda\" + str(\n",
    "#     lam) + \"Epsilon\" + str(agent.epsilon) + \"Episodes\" + str(num_of_episodes)  # 可修改\n",
    "# if env.walls == []:\n",
    "#     output_dir = output_dir + \"NoWalls\"\n",
    "# else:\n",
    "#     output_dir = output_dir + \"Walls\"\n",
    "# mkdir_p(output_dir)\n",
    "\n",
    "output_dir = folder_cluster_layout\n",
    "## Reward\n",
    "whenConverged = []\n",
    "toPickle = []\n",
    "plt.figure(1)\n",
    "plotRewards = np.mean(flags_list_episodes_experiments_repetitions, axis=0)\n",
    "plotSDs = np.std(flags_list_episodes_experiments_repetitions, axis=0)\n",
    "print(\"plotRewards.shape\", plotRewards.shape)\n",
    "print(\"plotSDs.shape\", plotSDs.shape)\n",
    "plotErrors = plotSDs / np.sqrt(10)\n",
    "plt.rcParams['agg.path.chunksize'] = 10000\n",
    "for i in range(0, len(plotRewards)):\n",
    "    d = pd.Series(plotRewards[i])\n",
    "    s = pd.Series(plotErrors[i])\n",
    "    movAv = pd.Series.rolling(d, window=int(num_of_episodes / 10), center=False).mean()\n",
    "    toPickle.append(movAv)\n",
    "    l, caps, c = plt.errorbar(np.arange(len(movAv)), movAv, label=labs[i], yerr=plotErrors[i], capsize=5, errorevery=int(num_of_episodes / 10))\n",
    "    for cap in caps:\n",
    "        cap.set_marker(\"_\")\n",
    "plt.ylabel(\"No. Of Flags Collected\")\n",
    "plt.xlabel(\"Episode No.\")\n",
    "plt.legend(loc=4)\n",
    "plt.axis([0, num_of_episodes, 0, 3])\n",
    "# print(whenConverged)\n",
    "\n",
    "# with open(\"{}/resultsListPickle\".format(output_dir), 'wb') as p:\n",
    "#     pickle.dump(toPickle, p)\n",
    "\n",
    "##plt.title(\"Number of Episodes: \" + str(num_of_episodes) + \" Alpha: \" + str(lr) + \" Gamma: \" + str(gamma) + \" Lambda: \" +str(lam) + \" Epsilon: \"+str(agent.epsilon))\n",
    "\n",
    "\n",
    "plt.savefig(\"{}/rewardGraph.png\".format(output_dir), dpi=1200, facecolor='w', edgecolor='w',\n",
    "            orientation='portrait', papertype=None, format=None,\n",
    "            transparent=False, bbox_inches=None, pad_inches=0.1)\n",
    "\n",
    "## Flags Collected\n",
    "plt.figure(2)\n",
    "plotFlags = np.mean(flags_list_episodes_experiments_repetitions, axis=0)\n",
    "plt.rcParams['agg.path.chunksize'] = 10000\n",
    "for i in range(0, len(plotFlags)):\n",
    "    d = pd.Series(plotFlags[i])\n",
    "    movAv = pd.Series.rolling(d, window=int(num_of_episodes / 10), center=False).mean()\n",
    "    plt.plot(np.arange(len(movAv)),movAv, label=labs[i])\n",
    "plt.ylabel(\"Number of Flags\")\n",
    "plt.xlabel(\"Episde No.\")\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.savefig(\"{}/rewardGraph_noerrorbar.png\".format(output_dir), dpi=1200, facecolor='w', edgecolor='w',\n",
    "            orientation='portrait', papertype=None, format=None,\n",
    "            transparent=False, bbox_inches=None, pad_inches=0.1)\n",
    "\n",
    "# plt.figure(3)\n",
    "# plotAbsTimings = np.mean(solve_amdp_time_experiments_repetitions, axis=0)\n",
    "# for i, v in enumerate(plotAbsTimings):\n",
    "#     plt.text(i - 0.25, v + 1.5, str(round(np.sum(v), 1)), color='blue', fontweight='bold')\n",
    "# plt.bar(np.arange(len(plotAbsTimings)), plotAbsTimings)\n",
    "# plt.xticks(np.arange(len(plotAbsTimings)), labs)\n",
    "# plt.xlabel(\"Abstraction Used\")\n",
    "# plt.ylabel(\"Time Taken\")\n",
    "# plt.title(\"Time Taken to Solve Each Abstraction\")\n",
    "# # plt.savefig(\"{}/AbstractionTime.png\".format(output_dir), dpi=1200, facecolor='w', edgecolor='w',\n",
    "# #             orientation='portrait', papertype=None, format=None,\n",
    "# #             transparent=False, bbox_inches=None, pad_inches=0.1)\n",
    "\n",
    "# plt.figure(4)\n",
    "# plotSimTimings = np.mean(simulation_time_experiments_repetitions, axis=0)\n",
    "# for i, v in enumerate(plotSimTimings):\n",
    "#     plt.text(i - 0.30, v + 1.5, str(round(v, 1)), color='blue', fontweight='bold')\n",
    "# plt.bar(np.arange(len(plotSimTimings)), plotSimTimings)\n",
    "# plt.xticks(np.arange(len(plotSimTimings)), labs)\n",
    "# plt.xlabel(\"Experiments\")\n",
    "# plt.ylabel(\"Time Taken In Seconds\")\n",
    "# plt.title(\"Time Taken To Simulate each experiment with episodes\" + str(num_of_episodes))\n",
    "# # plt.savefig(\"{}/SimulationTime.png\".format(output_dir), dpi=1200, facecolor='w', edgecolor='w',\n",
    "# #             orientation='portrait', papertype=None, format=None,\n",
    "# #             transparent=False, bbox_inches=None, pad_inches=0.1)\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}